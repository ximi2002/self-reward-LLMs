{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a535704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/yixin.wen/minghan.yu/.conda/envs/rl/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/blue/yixin.wen/minghan.yu/.conda/envs/rl/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-05 00:41:30,639] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The default cache directory for DeepSpeed Triton autotune, /home/minghan.yu/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.8\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.4.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/yixin.wen/minghan.yu/.conda/envs/rl/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_fwd\n",
      "/blue/yixin.wen/minghan.yu/.conda/envs/rl/lib/python3.11/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @autocast_custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, TrainingArguments, Trainer, get_cosine_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_bleu\n",
    "from peft import PeftModel\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from utils import *\n",
    "from datasets import Dataset as HFDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d40b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"models/ThinkLite-VL-7B\")\n",
    "sft_training_path = Path(\"data/alpaca_en_mini.json\")\n",
    "sft_adaptor_path = Path(\"checkpoints/sft_adaptor\")\n",
    "raw_response_path = Path(\"data/raw_responses.jsonl\")\n",
    "dpo_training_path = Path(\"data/dpo_training.jsonl\")\n",
    "dpo_model_path = Path(\"checkpoints/dpo_no_self_instruction\")\n",
    "test_path = Path(\"data/alpaca_en_test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2bbac1",
   "metadata": {},
   "source": [
    "# Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f4cafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.75s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091df298",
   "metadata": {},
   "source": [
    "# Use LoRA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.config.use_cache = False\n",
    "base_model.gradient_checkpointing_enable()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\n",
    "        'q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "\n",
    "policy_model = get_peft_model(base_model, lora_config)\n",
    "policy_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d85f6",
   "metadata": {},
   "source": [
    "## Load Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfdc0e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 samples from /blue/yixin.wen/minghan.yu/self-rewarding-lm-pytorch/data/alpaca_en_mini.json\n",
      "Instruction in sample 0: Describe a process of making crepes.\n",
      "Input in sample 0: \n",
      "Output in sample 0: Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:\n",
      "\n",
      "1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.\n",
      "\n",
      "2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.\n",
      "\n",
      "3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.\n",
      "\n",
      "4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.\n",
      "\n",
      "5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.\n",
      "\n",
      "6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.\n",
      "\n",
      "7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.\n",
      "\n",
      "8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy!\n",
      "num test samples: 200\n",
      "test sample:  [{'role': 'user', 'content': 'Write three lines of dialogue that best reflect the scene\\n\\nTwo friends reuniting after a long time'}, {'role': 'assistant', 'content': 'Friend 1: \"Oh my god, Sarah?! Is that you?\"\\nFriend 2: \"Linda! I can\\'t believe it! It\\'s been so long!\"\\nFriend 1: \"I know, it\\'s been years! You look amazing, how have you been?\"'}]\n"
     ]
    }
   ],
   "source": [
    "train_json = Path(sft_training_path)\n",
    "with train_json.open('r', encoding='utf-8') as f:\n",
    "    raw_samples = json.load(f)\n",
    "\n",
    "print(f'Loaded {len(raw_samples)} samples from {train_json}')\n",
    "print('Instruction in sample 0:', raw_samples[0]['instruction'])\n",
    "print('Input in sample 0:', raw_samples[0]['input'])\n",
    "print('Output in sample 0:', raw_samples[0]['output'])\n",
    "\n",
    "test_file = Path(test_path)\n",
    "with test_file.open('r', encoding='utf-8') as f:\n",
    "    test_samples = json.load(f)\n",
    "for i in range(len(test_samples)):\n",
    "    test_samples[i] = alpaca_to_sharegpt(test_samples[i])\n",
    "print('num test samples:', len(test_samples))\n",
    "print('test sample: ',test_samples[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec1f0f",
   "metadata": {},
   "source": [
    "# SFT The Seed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8e4cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready with 20 samples. First messages example:\n",
      "[{'role': 'user', 'content': 'Describe a process of making crepes.'}, {'role': 'assistant', 'content': \"Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:\\n\\n1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.\\n\\n2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.\\n\\n3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.\\n\\n4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.\\n\\n5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.\\n\\n6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.\\n\\n7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.\\n\\n8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy!\"}]\n"
     ]
    }
   ],
   "source": [
    "class TextSFTDataset(Dataset):\n",
    "    def __init__(self, data: List[Dict[str, Any]]):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "        return {\"messages\": alpaca_to_sharegpt(ex)}\n",
    "class TextSFTCollator:\n",
    "    def __init__(self, processor, max_length: int = 4096):\n",
    "        self.processor = processor\n",
    "        self.tokenizer = processor.tokenizer if hasattr(processor, \"tokenizer\") else processor\n",
    "        self.max_length = max_length\n",
    "        if self.tokenizer.pad_token_id is None and self.tokenizer.eos_token_id is not None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "    def __call__(self, batch: List[Dict[str, Any]]):\n",
    "        conversations = [item[\"messages\"] for item in batch]\n",
    "        texts = [\n",
    "            self.processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
    "            for msgs in conversations\n",
    "        ]\n",
    "        enc = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=self.max_length is not None,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        labels = enc[\"input_ids\"].clone()\n",
    "        pad_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
    "        if pad_id is not None:\n",
    "            labels[labels == pad_id] = -100\n",
    "        enc[\"labels\"] = labels\n",
    "        return enc\n",
    "sft_dataset = TextSFTDataset(raw_samples)\n",
    "sft_collator = TextSFTCollator(processor, max_length=4096)\n",
    "print(f\"Dataset ready with {len(sft_dataset)} samples. First messages example:\")\n",
    "print(sft_dataset[0][\"messages\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f167b65",
   "metadata": {},
   "source": [
    "## LoRA SFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1379c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/20269628/ipykernel_1933683/1134410044.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Trainer ready.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.655700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.678800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.043700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=3.340950036048889, metrics={'train_runtime': 21.0123, 'train_samples_per_second': 0.952, 'train_steps_per_second': 0.238, 'total_flos': 190119641530368.0, 'train_loss': 3.340950036048889, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bf16_flag = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "fp16_flag = torch.cuda.is_available() and not bf16_flag\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='outputs/thinklite_vision_lora_sft',\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5.5e-6,\n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    save_strategy='no',\n",
    "    report_to=[],\n",
    "    remove_unused_columns=False,\n",
    "    bf16=bf16_flag,\n",
    "    fp16=fp16_flag\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=policy_model,\n",
    "    args=training_args,\n",
    "    train_dataset=sft_dataset,\n",
    "    data_collator=sft_collator,\n",
    "    tokenizer=processor.tokenizer\n",
    ")\n",
    "\n",
    "print('LoRA Trainer ready.')\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf02f25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/blue/yixin.wen/minghan.yu/self-rewarding-lm-pytorch/checkpoints/sft_adaptor/tokenizer_config.json',\n",
       " '/blue/yixin.wen/minghan.yu/self-rewarding-lm-pytorch/checkpoints/sft_adaptor/special_tokens_map.json',\n",
       " '/blue/yixin.wen/minghan.yu/self-rewarding-lm-pytorch/checkpoints/sft_adaptor/vocab.json',\n",
       " '/blue/yixin.wen/minghan.yu/self-rewarding-lm-pytorch/checkpoints/sft_adaptor/merges.txt',\n",
       " '/blue/yixin.wen/minghan.yu/self-rewarding-lm-pytorch/checkpoints/sft_adaptor/added_tokens.json',\n",
       " '/blue/yixin.wen/minghan.yu/self-rewarding-lm-pytorch/checkpoints/sft_adaptor/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save LoRA adapter after SFT\n",
    "policy_model.save_pretrained(sft_adaptor_path.as_posix())\n",
    "processor.tokenizer.save_pretrained(sft_adaptor_path.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c055b0b",
   "metadata": {},
   "source": [
    "## SFT Test Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating sample 1/5\n",
      "user input: What are some of the key characteristics of a competitive market?\n",
      "reference: A competitive market is characterized by the presence of multiple buyers and sellers competing with each other to buy and sell goods and services at the best possible price. Some of the key characteristics of a competitive market are:\n",
      "\n",
      "1. Large number of buyers and sellers: In a competitive market, there are a large number of buyers and sellers, and no single buyer or seller can influence the market price.\n",
      "\n",
      "2. Homogeneous products: The products offered by different firms in a competitive market are similar, if not identical. This makes it easy for buyers to compare prices and make a purchase decision.\n",
      "\n",
      "3. Freedom of entry and exit: Firms can freely enter or exit the market, which ensures that new firms can enter the market if existing firms are making profits, and loss-making firms can exit the market.\n",
      "\n",
      "4. Perfect information: In a competitive market, buyers and sellers have access to complete information about the products, prices, and other relevant factors. This allows them to make informed decisions.\n",
      "\n",
      "5. Price takers: Both buyers and sellers in a competitive market are price takers, meaning they must accept the market price and cannot influence it.\n",
      "\n",
      "These characteristics result in a high level of competition, lower prices, and greater efficiency, as firms strive to provide the best products and services at the lowest possible price to attract customers.\n",
      "Evaluating sample 2/5\n",
      "user input: List five characteristics of good communication.\n",
      "reference: 1. Clarity: Good communication is clear and easy to understand. It conveys the intended message without ambiguity, and the sender ensures that their message is received accurately by the recipient.\n",
      "2. Conciseness: Effective communication is concise and to the point, avoiding unnecessary repetition or elaboration. It is focused and effectively conveys the most important information.\n",
      "3. Active Listening: Good communication involves active listening, where the recipient is fully engaged, and actively trying to understand the message.\n",
      "4. Empathy: Effective communication involves empathy, where the sender and recipient are able to understand each other’s feelings and perspectives, and communicate with respect and sensitivity.\n",
      "5. Feedback: Good communication involves giving and receiving feedback, to ensure that the intended message has been received correctly, and to allow for clarifications and improvements in future communication.\n",
      "Evaluating sample 3/5\n",
      "user input: Create instructions for defrosting frozen chicken.\n",
      "reference: Here are instructions for defrosting frozen chicken: \n",
      "\n",
      "1. Plan ahead: Keep in mind that defrosting chicken takes time, so plan ahead to ensure the chicken is thoroughly defrosted before cooking. Depending on the method you choose, defrosting can take anywhere from a few hours to a day. \n",
      "\n",
      "2. Use the refrigerator method: The safest way to defrost chicken is to place it in the refrigerator. This method allows the chicken to thaw slowly, reducing the risk of bacterial growth.  Place the frozen chicken in a dish or a plate to catch any liquids that might leak out and put it on the lowest shelf in your refrigerator. Allow 5 hours of defrosting time per pound of chicken. \n",
      "\n",
      "3. Use the cold water method: If you need to defrost the chicken faster, you can use the cold water method. Place the chicken in a leak-proof plastic bag, then submerge it in a bowl or sink full of cold water. Change the water every 30 minutes to make sure it stays cold. Allow 1 hour of defrosting time per pound of chicken.\n",
      "\n",
      "4. Use the microwave method: If you are in a hurry, you can use the microwave to defrost the chicken. Place the chicken in a microwave-safe dish and use the defrost setting. Check the chicken frequently to make sure it is defrosting evenly and not starting to cook in some parts. Make sure to cook the chicken immediately after defrosting in the microwave.\n",
      "\n",
      "5. Do not defrost at room temperature: It is not safe to defrost chicken at room temperature, as this can encourage the growth of harmful bacteria. Always use one of the methods above to ensure the chicken is safe to eat.\n",
      "\n",
      "6. Cook thoroughly: Once the chicken is defrosted, cook it thoroughly to an internal temperature of at least 165°F (74°C) to kill any bacteria that might be present.\n",
      "Evaluating sample 4/5\n",
      "user input: Write three lines of dialogue that best reflect the scene\n",
      "\n",
      "Two friends reuniting after a long time\n",
      "reference: Friend 1: \"Oh my god, Sarah?! Is that you?\"\n",
      "Friend 2: \"Linda! I can't believe it! It's been so long!\"\n",
      "Friend 1: \"I know, it's been years! You look amazing, how have you been?\"\n",
      "Evaluating sample 5/5\n",
      "user input: Assign the following topics to each sentence\n",
      "\n",
      "1. The fastest mammal on land is the cheetah.\n",
      "2. Cheetahs can run up to 70 mph.\n",
      "reference: 1. Topic: Land Mammals\n",
      "2. Topic: Cheetah Abilities\n",
      "Aggregated Metrics:\n",
      "ROUGE-1: 0.00\n",
      "ROUGE-2: 0.00\n",
      "ROUGE-L: 0.00\n",
      "BLEU-4: 0.00\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_generation_metrics(\n",
    "    model=policy_model,      \n",
    "    processor=processor,\n",
    "    test_samples=test_samples,\n",
    "    max_new_tokens=128,\n",
    "    device=policy_model.device\n",
    ")\n",
    "print(\"Aggregated Metrics:\")\n",
    "print(f\"ROUGE-1: {results['rouge1_mean']:.2f}\")\n",
    "print(f\"ROUGE-2: {results['rouge2_mean']:.2f}\")\n",
    "print(f\"ROUGE-L: {results['rougeL_mean']:.2f}\")\n",
    "print(f\"BLEU-4: {results['bleu4']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa719181",
   "metadata": {},
   "source": [
    "# Generate DPO Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9d17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf_device_map: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Describe a process of making crepes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:   5%|▌         | 1/20 [01:51<35:25, 111.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Transform the following sentence using a synonym: The car sped quickly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  10%|█         | 2/20 [02:16<18:07, 60.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Make a persuasive argument to promote recycling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  15%|█▌        | 3/20 [03:50<21:31, 75.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Invent a new word by combining two existing words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  20%|██        | 4/20 [04:20<15:23, 57.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Give an example of a job that a computer can do better than a human being.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  25%|██▌       | 5/20 [05:04<13:12, 52.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Given the parameters of a triangle, find out its perimeter.\n",
      "\n",
      "Side 1 = 4\n",
      "Side 2 = 6\n",
      "Side 3 = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  30%|███       | 6/20 [05:54<12:07, 51.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Create an effective 140 character twitter post\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  35%|███▌      | 7/20 [06:23<09:35, 44.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Produce a list of the top 5 NHL players in 2021.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  40%|████      | 8/20 [08:00<12:12, 61.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Reword this sentence to increase clarity\n",
      "\n",
      "The idea of her being so brave made me smile\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  45%|████▌     | 9/20 [08:25<09:08, 49.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Explain the differences between birds and mammals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  50%|█████     | 10/20 [10:19<11:37, 69.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Generate a one-sentence title for a creative recipe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  55%|█████▌    | 11/20 [10:46<08:30, 56.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Explain the concept of e-commerce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  60%|██████    | 12/20 [12:13<08:45, 65.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Design an experiment to evaluate the efficacy of the proposed method.\n",
      "\n",
      "Proposed Method: Neural persistence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  65%|██████▌   | 13/20 [14:09<09:27, 81.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Generate a list of five different books about science.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  70%|███████   | 14/20 [15:10<07:29, 74.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Brainstorm some activities that could make an in-person work meeting more engaging.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  75%|███████▌  | 15/20 [16:58<07:05, 85.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Brainstorm a list of titles for a photo album\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  80%|████████  | 16/20 [17:53<05:03, 75.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Rewrite the sentence so that it's in the present tense.\n",
      "\n",
      "She had worked at the company for the past 3 years.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  85%|████████▌ | 17/20 [18:18<03:01, 60.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Adapt the provided joke to make it more humorous.\n",
      "\n",
      "Why did the frog cross the road?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  90%|█████████ | 18/20 [18:45<01:41, 50.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Create an AI chatbot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data:  95%|█████████▌| 19/20 [20:42<01:10, 70.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Explain what a circuit breaker is.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building self-rewarding DPO data: 100%|██████████| 20/20 [21:57<00:00, 65.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed 11 DPO samples (after filtering).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "policy_model.eval()\n",
    "device = policy_model.device\n",
    "print('model_device:', device)\n",
    "\n",
    "\n",
    "PROMPT_GEN_TEMPLATE = (\n",
    "    \"You are an expert meteorologist creating challenging user instructions that require analyzing weather maps.\"\n",
    "    \"Below are example instructions (After User:) and responses(After Assistant:). Write a new instruction in a similar style that asks for a structured weather analysis.\"\n",
    "    \"Return only the new instruction. Do not include 'User:'. The new instruction should be concise and not exceed 30 words. \"\n",
    "    \"{few_shot_block}\"\n",
    "    \"New instruction:\"\n",
    ")\n",
    "\n",
    "REWARD_PROMPT_TEMPLATE = (\n",
    "    \"Review the user's question and the corresponding response using the additive 5-point scoring system described below. \"\n",
    "    \"Points are accumulated based on the satisfaction of each criterion: - Add 1 point if the response is relevant and provides some information related to the user's inquiry, even if it is incomplete or contains some irrelevant content. \"\n",
    "    \"- Add another point if the response addresses a substantial portion of the user's question, but does not completely resolve the query or provide a direct answer. \"\n",
    "    \"- Award a third point if the response answers the basic elements of the user's question in a useful way, regardless of whether it seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results. \"\n",
    "    \"- Grant a fourth point if the response is clearly written from an AI Assistant's perspective, addressing the user's question directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity, conciseness or focus. \"\n",
    "    \"- Bestow a fifth point for a response that is impeccably tailored to the user's question by an AI Assistant, without extraneous information, reflecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer. \"\n",
    "    \"User: {instruction}<response>{response}</response>\"\n",
    "    \"After examining the user's instruction and the response:\"\n",
    "    \"- Briefly justify your total score, up to 100 words.\"\n",
    "    \"- Conclude with the score using the format: Score: <total points>\"\n",
    "    \"Remember to assess from the AI Assistant perspective, utilizing web search knowledge as necessary. To evaluate the response in alignment with this additive scoring model, we will systematically attribute points based on the outlined criteria.\"\n",
    ")\n",
    "\n",
    "reward_score_regex = re.compile(r\"Score:\\s*([0-5](?:\\.\\d+)?)\")\n",
    "num_candidates = 4\n",
    "\n",
    "dpo_records = []\n",
    "random.seed(42)\n",
    "\n",
    "for entry in tqdm(sft_dataset, desc='Building self-rewarding DPO data'):\n",
    "    instruction = entry[\"messages\"][0][\"content\"]\n",
    "    print(\"Instruction:\", instruction)\n",
    "    candidate_responses = generate_candidates(  \n",
    "        model=policy_model,\n",
    "        processor=processor,\n",
    "        instruction=instruction,\n",
    "        num_candidates=num_candidates,\n",
    "        generation_kwargs=dict(max_new_tokens=512, temperature=0.7, top_p=0.9, do_sample=True),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    scored_candidates = []\n",
    "    for candidate in candidate_responses:\n",
    "        score, judgement = judge_response(model=policy_model, tokenizer=processor, instruction=instruction, response=candidate, reward_prompt_template=REWARD_PROMPT_TEMPLATE, reward_score_regex=reward_score_regex, n_votes=3, device=device)\n",
    "        scored_candidates.append({\"response\": candidate, \"score\": score, \"judge_output\": judgement})\n",
    "\n",
    "    valid_candidates = [c for c in scored_candidates if not math.isnan(c['score'])]  # drop failed parses\n",
    "    if len(valid_candidates) < 2:\n",
    "        continue\n",
    "\n",
    "    sorted_candidates = sorted(valid_candidates, key=lambda c: c['score'], reverse=True)  # best-to-worst\n",
    "    best = sorted_candidates[0]\n",
    "    worst = sorted_candidates[-1]\n",
    "\n",
    "    dpo_records.append({  # assemble final DPO entry\n",
    "        'instruction': instruction,\n",
    "        'candidates': scored_candidates,\n",
    "        'chosen': best['response'],\n",
    "        'chosen_score': best['score'],\n",
    "        'rejected': worst['response'],\n",
    "        'rejected_score': worst['score']\n",
    "    })\n",
    "\n",
    "print(f'Constructed {len(dpo_records)} DPO samples (after filtering).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb877a41",
   "metadata": {},
   "source": [
    "## Save and Preview DPO Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "337098e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 11 pairs to /blue/yixin.wen/minghan.yu/self-rewarding-lm-pytorch/data/raw_responses.jsonl\n",
      "Sample entry keys: ['instruction', 'candidates', 'chosen', 'chosen_score', 'rejected', 'rejected_score']\n",
      "Chosen score: 4.5 Rejected score: 4.0\n"
     ]
    }
   ],
   "source": [
    "with raw_response_path.open('w', encoding='utf-8') as f:\n",
    "    for item in dpo_records:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f'Saved {len(dpo_records)} pairs to {raw_response_path}')\n",
    "\n",
    "with raw_response_path.open('r', encoding='utf-8') as f:\n",
    "    first_line = f.readline().strip()\n",
    "    if first_line:\n",
    "        preview = json.loads(first_line)\n",
    "        print('Sample entry keys:', list(preview.keys()))\n",
    "        print('Chosen score:', preview.get('chosen_score'), 'Rejected score:', preview.get('rejected_score'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220a1f0",
   "metadata": {},
   "source": [
    "## Clean DPO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "234e5d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[done] total=11, kept=10, had_nan_candidates=4, dropped_tie=1, dropped_other=0\n",
      "[out] /blue/yixin.wen/minghan.yu/self-rewarding-lm-pytorch/data/dpo_training.jsonl\n"
     ]
    }
   ],
   "source": [
    "dpo_training_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with_scores = False\n",
    "TIE_EPS = 1e-6\n",
    "total = kept = dropped_nan = dropped_tie = dropped_other = 0\n",
    "\n",
    "with dpo_training_path.open(\"w\", encoding=\"utf-8\") as g:\n",
    "    for rec in iter_records(raw_response_path):\n",
    "        total += 1\n",
    "        # Count the number of NaN scores in the original candidates (for overview)\n",
    "        cands = rec.get(\"candidates\", [])\n",
    "        if isinstance(cands, list) and any(to_float(c.get(\"score\")) is None for c in cands):\n",
    "            dropped_nan += 1\n",
    "        dpo = build_dpo_entry(rec, include_scores=with_scores)\n",
    "        if dpo is None:\n",
    "            # Determine the reason (as best as possible)\n",
    "            if not rec.get(\"candidates\") or len(rec.get(\"candidates\")) < 2:\n",
    "                dropped_other += 1\n",
    "            else:\n",
    "                # Re-evaluate if it's a tie\n",
    "                vals = [to_float(c.get(\"score\")) for c in rec[\"candidates\"]]\n",
    "                vals = [v for v in vals if v is not None]\n",
    "                if len(vals) >= 2 and (max(vals) - min(vals) <= TIE_EPS):\n",
    "                    dropped_tie += 1\n",
    "                else:\n",
    "                    dropped_other += 1\n",
    "            continue\n",
    "            # Write out the valid DPO entry\n",
    "        g.write(json.dumps(dpo, ensure_ascii=False) + \"\\n\")\n",
    "        kept += 1\n",
    "\n",
    "print(f\"[done] total={total}, kept={kept}, \"\n",
    "      f\"had_nan_candidates={dropped_nan}, dropped_tie={dropped_tie}, dropped_other={dropped_other}\")\n",
    "print(f\"[out] {dpo_training_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba728a5",
   "metadata": {},
   "source": [
    "# DPO Training (Reference model is replaced by a different Lora adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/yixin.wen/minghan.yu/.conda/envs/rl/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use the previous LoRA as the reference model, then update the current LoRA as the policy model\n",
    "previous_adaptor_path = sft_adaptor_path  \n",
    "current_adaptor_path = sft_adaptor_path  \n",
    "policy_model = load_policy_with_lora(base_model, current_adaptor_path)\n",
    "ref_model = load_policy_with_lora(base_model, previous_adaptor_path)\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2250c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/yixin.wen/minghan.yu/.conda/envs/rl/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_prompt_length, max_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\n",
      "/blue/yixin.wen/minghan.yu/.conda/envs/rl/lib/python3.11/site-packages/trl/trainer/dpo_trainer.py:389: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/blue/yixin.wen/minghan.yu/.conda/envs/rl/lib/python3.11/site-packages/trl/trainer/dpo_trainer.py:402: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 raw pairs from /blue/yixin.wen/minghan.yu/self-rewarding-lm-pytorch/data/dpo_training.jsonl\n",
      "First formatted row: {'prompt': '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nDescribe a process of making crepes.<|im_end|', 'chosen': \"Making crepes is a delightful culinary experience that requires a bit of patience and practice. Here's a step-by-step gu\", 'rejected': \"Making crepes is a delightful culinary process that can be both fun and satisfying. Here's a step-by-step guide to makin\"}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(raw_dpo_pairs)} raw pairs from {dpo_training_path}\")\n",
    "rows = []\n",
    "for rec in raw_dpo_pairs:\n",
    "    prompt = rec.get(\"prompt\") or rec.get(\"instruction\") or rec.get(\"query\")\n",
    "    chosen, rejected = rec.get(\"chosen\"), rec.get(\"rejected\")\n",
    "    if not prompt or not chosen or not rejected:\n",
    "        continue\n",
    "    rows.append({\"prompt\": to_chat_prompt(prompt), \"chosen\": chosen, \"rejected\": rejected})\n",
    "dpo_dataset = HFDataset.from_list(rows)\n",
    "print(\"First formatted row:\", {k: v[:120] for k, v in dpo_dataset[0].items()})\n",
    "# safety: set precision flags if not already defined\n",
    "if \"bf16_flag\" not in globals():\n",
    "    bf16_flag = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8\n",
    "if \"fp16_flag\" not in globals():\n",
    "    fp16_flag = torch.cuda.is_available() and not bf16_flag\n",
    "dpo_args = DPOConfig(\n",
    "    output_dir=dpo_model_path.as_posix(),\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-6,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[],\n",
    "    remove_unused_columns=False,\n",
    "    bf16=bf16_flag,\n",
    "    fp16=fp16_flag,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=policy_model,      \n",
    "    ref_model=ref_model,    \n",
    "    args=dpo_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=dpo_dataset,\n",
    "    tokenizer=processor,\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    ")\n",
    "print(\"Starting DPO training from SFT LoRA...\")\n",
    "dpo_trainer.train()\n",
    "policy_model.save_pretrained(dpo_model_path.as_posix())\n",
    "processor.save_pretrained(dpo_model_path.as_posix())\n",
    "print(f\"Saved DPO-tuned adapter to {dpo_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dfed72",
   "metadata": {},
   "source": [
    "# Test Acc again Again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated Metrics:\n",
      "ROUGE-1: 4.71\n",
      "ROUGE-2: 0.61\n",
      "ROUGE-L: 3.46\n",
      "BLEU-4: 0.40\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_generation_metrics(\n",
    "    model=policy_model,      \n",
    "    processor=processor,\n",
    "    test_samples=test_samples,\n",
    "    max_new_tokens=128,\n",
    "    device=policy_model.device\n",
    ")\n",
    "print(\"Aggregated Metrics:\")\n",
    "print(f\"ROUGE-1: {results['rouge1_mean']:.2f}\")\n",
    "print(f\"ROUGE-2: {results['rouge2_mean']:.2f}\")\n",
    "print(f\"ROUGE-L: {results['rougeL_mean']:.2f}\")\n",
    "print(f\"BLEU-4: {results['bleu4']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8ab13",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
